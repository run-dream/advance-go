## 隔离

### 定义

隔离，本质上是对系统或资源进行分割，从而实现当系统发生故障时能限定传播范围和影响范围，即发生故障后只有出问题的服务不可用，保证其他服务仍然可用。

### 分类

- 服务隔离
  - 动静分离
  - 读写分离
- 轻重隔离
  - 核心
  - 快慢
  - 热点
- 物理隔离
  - 线程
  - 进程
  - 集群
  - 机房

### 具体实践

- 动静分离

  将动态和静态资源进行分离。这样可以对访问变换频次小的资源进行缓存。

  - cpu cacheline -> false sharing
  - mysql bufferpool
  - 图片、html 静态资源 
  - CDN 将静态资源和动态 API 分离

- 读写分离

  - 主从
  - Replicaset
  - CQRS

- 核心隔离

  业务按照 Level 进行资源池划分（L0/L1/L2）

  - 核心/非核心的故障域的差异隔离（机器资源、依赖资源）
  - 多集群，通过冗余资源来提升吞吐和容灾能力。

- 快慢隔离

  我们可以把服务的吞吐想象为一个池，当突然洪流进来时，池子需要一定时间才能排放完，这时候其他支流在池子里待的时间取决于前面的排放能力，耗时就会增高，对小请求产生影响。

  消息队列 kafka 的消费也满足这个定义。

  - 按照各种纬度隔离：sink、部门、业务、logid、重要性（S/A/B/C）

- 热点隔离

  热点即经常访问的数据。很多时候我们希望统计某个热点数据中访问频次最高的 Top K 数据，并对其访问进行缓存。

  - 小表广播
  - 主动预热

- 线程隔离

  主要通过线程池进行隔离，也是实现服务隔离的基础。把业务进行分类并交给不同的线程池进行处理，当某个线程池处理一种业务请求发生问题时，不会讲故障扩散和影响到其他线程池，保证服务可用。

  golang 中 由语言本身控制。

- 进程隔离

  容器化（docker），容器编排引擎（k8s）

- 集群隔离

### 案例

- INFO 日志量过大，导致异常 ERROR 日志采集延迟
- 数据库实例 cgroup 未隔离，导致大 SQL 引起的集体故障
- 缩略图服务，被大图实时缩略吃完所有 CPU，导致正常的小图缩略被丢弃，大量503



## 超时控制

超时控制，我们的组件能够快速失效（fail fast），因为我们不希望等到断开的实例直到超时。没有什么比挂起的请求和无响应的界面更令人失望。这不仅浪费资源，而且还会让用户体验变得更差。我们的服务是互相调用的，所以在这些延迟叠加前，应该特别注意防止那些超时的操作。

1. 网络传递具有不确定性
2. 客户端和服务端不一致的超时策略导致资源浪费
3. “默认值”策略
4. 高延迟服务导致 client 浪费资源等待，使用超时传递: 进程间传递 + 跨进程传递

实际业务开发中，我们依赖的微服务的超时策略并不清楚，或者随着业务迭代耗时超生了变化，意外的导致依赖者出现了超时。

避免出现意外的默认超时策略，或者意外的配置超时策略。
kit 基础库兜底默认超时，比如 100ms，进行配置防御保护，避免出现类似 60s 之类的超大超时策略。
配置中心公共模版，对于未配置的服务使用公共配置。

### 超时传递

当上游服务已经超时返回 504，但下游服务仍然在执行，会导致浪费资源做无用功。超时传递指的是把当前服务的剩余 Quota 传递到下游服务中，继承超时策略，控制请求级别的全局超时控制。

- 进程内超时控制

  一个请求在每个阶段(网络请求)开始前，就要检查是否还有足够的剩余来处理请求，以及继承他的超时策略。

  在需要强制执行时，下游的服务可以覆盖上游的超时传递和配额。
  在 gRPC 框架中，会依赖 gRPC Metadata Exchange，基于 HTTP2 的 Headers 传递 grpc-timeout 字段，自动传递到下游，构建带 timeout 的 context。

- 双峰分布: 95%的请求耗时在100ms内，5%的请求可能永远不会完成（长超时）。
  对于监控不要只看 mean，可以看看耗时分布统计，比如 95th，99th。

- 设置合理的超时，拒绝超长请求，或者当Server 不可用要主动失败。

**超时决定着服务线程耗尽**



### 案例

- SLB 入口 Nginx 没配置超时导致连锁故障。
服务依赖的 DB 连接池漏配超时，导致请求阻塞，最终服务集体 OOM。
下游服务发版耗时增加，而上游服务配置超时过短，导致上游请求失败。



## 过载保护

### 被动保护

#### 令牌桶算法

是一个存放固定容量令牌的桶，按照固定速率往桶里添加令牌。令牌桶算法的描述如下：

1. 假设限制2r/s，则按照500毫秒的固定速率往桶中添加令牌。
2. 桶中最多存放 b 个令牌，当桶满时，新添加的令牌被丢弃或拒绝。
3. 当一个 n 个字节大小的数据包到达，将从桶中删除n 个令牌，接着数据包被发送到网络上。
4. 如果桶中的令牌不足 n 个，则不会删除令牌，且该数据包将被限流（要么丢弃，要么缓冲区等待）。

[参考实现](https://github.com/run-dream/limit-req/blob/main/src/app/lib/limit_req/acquire.lua)

#### 漏桶算法

作为计量工具时，可以用于流量整形和流量控制，漏桶算法的描述如下：

1. 一个固定容量的漏桶，按照常量固定速率流出水滴。
2. 如果桶是空的，则不需流出水滴。
3. 可以以任意速率流入水滴到漏桶。
4. 如果流入水滴超出了桶的容量，则流入的水滴溢出了（被丢弃），而漏桶容量是不变的。

### 存在的问题

漏斗桶/令牌桶确实能够保护系统不被拖垮, 但不管漏斗桶还是令牌桶, 其防护思路都是设定一个**指标**, 当超过该指标后就阻止或减少流量的继续进入，当系统负载降低到某一水平后则恢复流量的进入。但其通常都是被动的，其实际效果取决于**限流阈值设置**是否合理，但往往设置合理不是一件容易的事情。

- 集群增加机器或者减少机器限流阈值是否要重新设置?
- 设置限流阈值的依据是什么?
- 人力运维成本是否过高?
- 当调用方反馈429时, 这个时候重新设置限流, 其实流量高峰已经过了重新评估限流是否有意义?

### 自适应限流（过载保护）

计算系统临近过载时的峰值吞吐作为限流的阈值来进行流量控制，达到系统保护。
服务器临近过载时，主动抛弃一定量的负载，目标是自保。在系统稳定的前提下，保持系统的吞吐量。

- 常见做法：利特尔法则 
  CPU、内存作为信号量进行节流。
  队列管理: 队列长度、LIFO。
  可控延迟算法: CoDel。

- 如何计算接近峰值时的系统吞吐？
  - CPU: 使用一个独立的线程采样，每隔 250ms 触发一次。在计算均值时，使用了简单滑动平均去除峰值的影响。
  - Inflight: 当前服务中正在进行的请求的数量。
  - Pass&RT: 最近5s，pass 为每100ms采样窗口内成功请求的数量，rt 为单个采样窗口中平均响应时间
- 如何判断是否过载？
  - 我们使用 CPU 的滑动均值（CPU > 80%）作为启发阈值，一旦触发进入到过载保护阶段，算法为：(pass* rt) < inflight
  - 限流效果生效后，CPU 会在临界值（80%）附近**抖动**，如果不使用冷却时间，那么一个短时间的 CPU 下降就可能导致大量请求被放行，严重时会打满 CPU。
  - 在冷却时间后，重新判断阈值（CPU > 80% ），是否持续进入过载保护。

## 限流

限流是指在一段时间内，定义某个客户或应用可以接收或处理多少个请求的技术。例如，通过限流，你可以过滤掉产生流量峰值的客户和微服务，或者可以确保你的应用程序在自动扩展失效前都不会出现过载的情况。

### 方式和缺点

- 令牌桶、漏桶 针对单个节点，无法分布式限流。
- QPS 限流
  - 不同的请求可能需要数量迥异的资源来处理。
  - 某种静态 QPS 限流不是特别准。
- 给每个用户设置限制
  - 全局过载发生时候，针对某些“异常”进行控制。
  - 一定程度的“超卖”配额。
- 按照优先级丢弃。
- 拒绝请求也需要成本。

### 分布式限流

缺点：

- 单个大流量的接口，使用 redis 容易产生热点。
- pre-request 模式对性能有一定影响，高频的网络往返。

思考：

从获取单个 quota 升级成批量 quota。quota: 表示速率，获取后使用令牌桶算法来限制。

解决：

- 每次心跳后，异步批量获取 quota，可以大大减少请求 redis 的频次，获取完以后本地消费，基于令牌桶拦截。

问题：

- 每次申请的配额需要手动设定静态值略欠灵活，比如每次要20，还是50。
- 如何基于单个节点按需申请，并且避免出现不公平的现象？

解决:

- 初次使用默认值，一旦有过去历史窗口的数据，可以基于历史窗口数据进行 quota 请求。

思考:

- 我们经常面临给一组用户划分稀有资源的问题，他们都享有等价的权利来获取资源，但是其中一些用户实际上只需要比其他用户少的资源。

解决：

最大最小公平分享。

直观上，公平分享分配给每个用户想要的可以满足的最小需求，然后将没有使用的资源均匀的分配给需要‘大资源’的用户。
最大最小公平分配算法的形式化定义如下：

- 资源按照需求递增的顺序进行分配。
- 不存在用户得到的资源超过自己的需求。
- 未得到满足的用户等价的分享资源。



### 重要性

每个接口配置阈值，运营工作繁重，最简单的我们配置服务级别 quota，更细粒度的，我们可以根据不同重要性设定 quota，我们引入了重要性（criticality）:

- **最重要** CRITICAL_PLUS，为最终的要求预留的类型，拒绝这些请求会造成非常严重的用户可见的问题。
- **重要** CRITICAL，生产任务发出的默认请求类型。拒绝这些请求也会造成用户可见的问题。但是可能没那么严重。
- **可丢弃的** SHEDDABLE_PLUS 这些流量可以容忍某种程度的不可用性。这是批量任务发出的请求的默认值。这些请求通常可以过几分钟、几小时后重试。
- **可丢弃的** SHEDDABLE 这些流量可能会经常遇到部分不可用情况，偶尔会完全不可用。

gRPC 系统之间，需要自动传递重要性信息。如果后端接受到请求 A，在处理过程中发出了请求 B 和 C 给其他后端，请求 B 和 C 会使用与 A 相同的重要性属性。

- 全局配额不足时，优先拒绝低优先级的。
- 全局配额，可以按照重要性分别设置。
- 过载保护时，低优先级的请求先被拒绝。



### 限流的方式

- 熔断 超过资源配额时，后端任务会快速拒绝请求，返回“配额不足”的错误 429
- gutter  把抛弃的流量转移到 gutter 集群，如果 gutter 也接受不住的流量，重新回抛到主集群，最大力度来接受
- 客户端流控 客户端需要限制请求频次，retry backoff 做一定的请求退让。



### 案例

- 二层缓存穿透、大量回源导致的核心服务故障。
- 异常客户端引起的服务故障（query of death）
  - 请求放大。
  - 资源数放大。
- 用户重试导致的大面积故障。

## 降级

通过降级回复来减少工作量，或者丢弃不重要的请求。而且需要了解哪些流量可以降级，并且有能力区分不同的请求。我们通常提供降低回复的质量来答复减少所需的计算量或者时间。我们自动降级通常需要考虑几个点：

- 确定具体采用哪个指标作为流量评估和优雅降级的决定性指标（如，CPU、延迟、队列长度、线程数量、错误等）。
- 当服务进入降级模式时，需要执行什么动作？
- 流量抛弃或者优雅降级应该在服务的哪一层实现？是否需要在整个服务的每一层都实现，还是可以选择某个高层面的关键节点来实现？

同时我们要考虑一下几点：

- 优雅降级不应该被经常触发 - 通常触发条件现实了容量规划的失误，或者是意外的负载。
- 演练，代码平时不会触发和使用，需要定期针对一小部分的流量进行演练，保证模式的正常。
- 应该足够简单。

降级本质为: 提供有损服务。

- UI 模块化，非核心模块降级。
  - BFF 层聚合 API，模块降级。
- 页面上一次缓存副本。
- 默认值、热门推荐等。
- 流量拦截 + 定期数据缓存(过期副本策略)。

处理策略

- 页面降级
- 延迟服务
- 写/读降级
- 缓存降级
- 抛异常
- 返回约定协议
- Mock 数据
- Fallback 处理


### 案例

- 客户端解析协议失败，app 奔溃。
- 客户端部分协议不兼容，导致页面失败。
- local cache 数据源缓存，发版失效 + 依赖接口故障，引起的白屏。
- 没有 playbook，导致的 MTTR 上升。

## 重试

当请求返回错误（例: 配额不足、超时、内部错误等），对于 backend 部分节点过载的情况下，倾向于立刻重试，但是需要留意重试带来的流量放大:

- 限制重试次数和基于重试分布的策略（重试比率: 10%）。
- 随机化、指数型递增的重试周期: exponential ackoff + jitter。
- client 测记录重试次数直方图，传递到 server，进行分布判定，交由 server 判定拒绝。
- **只应该在失败的这层进行重试**，当重试仍然失败，全局约定错误码“过载，无须重试”，避免级联重试。



### 案例

- Nginx upstream retry 过大，导致服务雪崩。
- 业务不幂等，导致的重试，数据重复。
  - 全局唯一 ID: 根据业务生成一个全局唯一 ID，在调用接口时会传入该 ID，接口提供方会从相应的存储系统比如 redis 中去检索这个全局 ID 是否存在，如果存在则说明该操作已经执行过了，将拒绝本次服务请求；否则将相应该服务请求并将全局 ID 存入存储系统中,之后包含相同业务 ID 参数的请求将被拒绝。
  - 去重表: 这种方法适用于在业务中有唯一标识的插入场景。比如在支付场景中，一个订单只会支付一次，可以建立一张去重表,将订单 ID 作为唯一索引。把支付并且写入支付单据到去重表放入一个事务中了，这样当出现重复支付时，数据库就会抛出唯一约束异常,操作就会回滚。这样保证了订单只会被支付一次。
  - 多版本并发控制: 适合对更新请求作幂等性控制,比如要更新商品的名字，这是就可以在更新的接口中增加一个版本号来做幂等性控制。
- 多层级重试传递，放大流量引起雪崩。

## 负载均衡

数据中心内部的负载均衡
在理想情况下，某个服务的负载会完全均匀地分发给所有的后端任务。在任何时刻，最忙和最不忙的节点永远消耗同样数量的CPU。
目标：

- 均衡的流量分发。
- 可靠的识别异常节点。
- scale-out，增加同质节点扩容。
- 减少错误，提高可用性。

我们发现在 backend 之间的 load 差异比较大：

- 每个请求的处理成本不同。
- 物理机环境的差异:
  - 服务器很难强同质性。
  - 存在共享资源争用（内存缓存、带宽、IO等）。
- 性能因素:
  - FullGC。
  - JVM JIT。

参考JSQ（最闲轮训）负载均衡算法带来的问题，缺乏的是服务端全局视图，因此我们目标需要综合考虑：负载+可用性。

### p2c 算法

随机选取的两个节点进行打分，选择更优的节点:

1. 选择 backend：CPU，client：health、inflight、latency 作为指标，使用一个简单的线性方程进行打分。
2. 对新启动的节点使用常量惩罚值（penalty），以及使用探针方式最小化放量，进行预热。
3. 打分比较低的节点，避免进入“永久黑名单”而无法恢复，使用统计衰减的方式，让节点指标逐渐恢复到初始状态(即默认值)。
4. 当前发出去的请求超过了 predict lagtency，就会加惩罚。
5. 指标计算结合 moving average，使用时间衰减，计算vt = v(t-1) * β + at * (1-β) ，β 为若干次幂的倒数即: Math.Exp((-span) / 600ms)

## 最佳实践

- 变更管理:
  70％的问题是由变更引起的，恢复可用代码并不总是坏事。
- 避免过载:
  过载保护、流量调度等。
- 依赖管理:
  任何依赖都可能故障，做 chaos monkey testing，注入故障测试。
- 优雅降级:
  有损服务，避免核心链路依赖故障。
- 重试退避:
  退让算法，冻结时间，API retry detail 控制策略。
- 超时控制:
  进程内 + 服务间 超时控制。
- 极限压测 + 故障演练。
  扩容 + 重启 + 消除有害流量。